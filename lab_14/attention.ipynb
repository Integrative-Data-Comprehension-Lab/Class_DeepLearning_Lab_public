{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention-based Seq2Seq Neural Machine Translation\n",
    "\n",
    "문장 번역(translation) 문제에 대해 좀 더 깊이 생각해 보겠습니다. 사람이 문장을 번역할 때에는 일반적으로 다음과 같은 과정을 거칩니다:\n",
    "1. <b>문장 전체 이해</b>: 먼저 원문 전체를 읽고, 전달하고자 하는 의미와 개념을 이해합니다.\n",
    "2. <b>순차적 번역 생성</b>: 그 후, 한 번에 한 단어씩 번역을 작성해 나갑니다.\n",
    "3. <b>Attention</b>: 번역을 진행하는 동안, 필요할 때마다 원문을 다시 참조하여 ‘현재 번역하고자 하는 단어’와 가장 관련이 깊은 부분에 주의(attention)를 기울입니다.\n",
    "\n",
    "지난 실습에서 구현한 seq2seq 모델 구조에서는 1번 단계를 Encoder구조를 이용하여, 2번 단계를 Decoder구조를 이용하여 수행합니다.\n",
    "\n",
    "이번 실습의 목표는 3번 단계 <b>Attention 메커니즘</b>을 Seq2Seq 모델에 구현하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import spacy\n",
    "\n",
    "from helpers import TranslationDataset, train_one_epoch, evaluate_one_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이전 실습과 동일하게, 프랑스어(French) 문장을 입력 받아 영어(English) 문장을 출력하는 기계 번역 모델을 직접 구현하고 학습해 보겠습니다.\n",
    "\n",
    "이번 실습자료는 아래 두 논문에서 제시한 모델 구조를 기반으로 합니다.\n",
    "1.  Bahdanau et al., [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) (2014)\n",
    "    - Seq2Seq에 <b>Attention</b>을 처음으로 도입한 연구\n",
    "2.  Luong et al., [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/pdf/1508.04025) (2015)\n",
    "    - Attention 기법을 보다 효율적이고 체계적으로 개선한 연구\n",
    "    - 이번 실습은 <b>Luong-style Attention</b>을 구현하는 데 중점을 둡니다.\n",
    "\n",
    "## Dataset\n",
    "번역 모델 학습을 위한 `TranslationDataset`은 각 샘플을 튜플 `(source_sentence, target_sentence)`형식으로 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "dataset = TranslationDataset(path_tsv = \"/datasets/NLP/eng-fra.txt\")\n",
    "\n",
    "test_size = int(0.1 * len(dataset))\n",
    "valid_size = int(0.1 * len(dataset))\n",
    "train_size = len(dataset) - test_size - valid_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, valid_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}, Test size: {len(test_dataset)}\\n\")\n",
    "\n",
    "for source_sentence, target_sentence in train_dataset:\n",
    "    print(\"Example source sentence:\", source_sentence)\n",
    "    print(\"Example target sentence:\", target_sentence)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy tokenizers\n",
    "프랑스어와 영어 문장 토큰화를 위해서는 `spaCy` 라이브러리에서 제공하는 언어별 tokenizer를 활용합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "spacy_fr = spacy.load('fr_core_news_sm')\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "tokenizer_french = get_tokenizer(lambda text: [tok.text.lower() for tok in spacy_fr.tokenizer(text)])\n",
    "tokenizer_english = get_tokenizer(lambda text: [tok.text.lower() for tok in spacy_en.tokenizer(text)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "Tokenizer로 분할된 토큰들을 바탕으로 각 언어별 `vocabulary`를 생성합니다.\n",
    "\n",
    "특수 토큰(special tokens)으로는 `<pad>`, `<unk>`, `<sos>`, `<eos>` 4가지를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"  # Padding Token\n",
    "UNK_TOKEN = \"<unk>\"  # Unknown Token (Out of Vocabulary)\n",
    "SOS_TOKEN = \"<sos>\"  # Start of Sentence\n",
    "EOS_TOKEN = \"<eos>\"  # End of Sentence\n",
    "\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
    "\n",
    "source_token_list = [tokenizer_french(source) for source, _ in train_dataset]\n",
    "target_token_list = [tokenizer_english(target) for _, target in train_dataset]\n",
    "\n",
    "source_vocab = build_vocab_from_iterator(source_token_list, specials=SPECIAL_TOKENS, min_freq = 2)\n",
    "target_vocab = build_vocab_from_iterator(target_token_list, specials=SPECIAL_TOKENS, min_freq = 2)\n",
    "\n",
    "source_vocab.set_default_index(source_vocab[UNK_TOKEN])\n",
    "target_vocab.set_default_index(target_vocab[UNK_TOKEN])\n",
    "\n",
    "print(f\"Vocab sizes → French: {len(source_vocab)}, English: {len(target_vocab)}\")\n",
    "\n",
    "print(\"Source vocab (French): \", source_vocab.get_itos()[:10])\n",
    "print(\"Target vocab (English): \", target_vocab.get_itos()[:10])\n",
    "\n",
    "PAD_TOKEN_IDX  = source_vocab[PAD_TOKEN]\n",
    "SOS_TOKEN_IDX  = source_vocab[SOS_TOKEN]\n",
    "EOS_TOKEN_IDX  = source_vocab[EOS_TOKEN]\n",
    "assert PAD_TOKEN_IDX == target_vocab[PAD_TOKEN]\n",
    "assert SOS_TOKEN_IDX == target_vocab[SOS_TOKEN]\n",
    "assert EOS_TOKEN_IDX == target_vocab[EOS_TOKEN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate Function\n",
    "텍스트 전처리 파이프라인은 다음과 같은 과정으로 구성됩니다.:\n",
    "- <b>Tokenization</b> : 각 문장을 해당 언어의 tokenizer를 이용해 토큰 단위로 분할합니다.\n",
    "- <b>Vocabulary 매핑</b>: 각 토큰을 해당 언어의 `Vocabulary`를 이용하여 정수 인덱스(token indices)로 변환합니다\n",
    "- 문장의 시작에는 `<sos>`토큰을, 문장의 끝에는 `<eos>`토큰을 추가해줍니다.\n",
    "- <b>Padding</b>: mini-batch내의 <b>가장 긴 시퀀스 길이</b>에 맞춰 padding을 수행합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def collate_seq2seq_batch(batch):\n",
    "    source_batch, target_batch = [], []\n",
    "    for source_sentence, target_sentence in batch:\n",
    "        source_indices = [SOS_TOKEN_IDX] + [source_vocab[t] for t in tokenizer_french(source_sentence)] + [EOS_TOKEN_IDX]\n",
    "        target_indices = [SOS_TOKEN_IDX] + [target_vocab[t] for t in tokenizer_english(target_sentence)] + [EOS_TOKEN_IDX]\n",
    "        source_batch.append(torch.tensor(source_indices, dtype=torch.long))\n",
    "        target_batch.append(torch.tensor(target_indices, dtype=torch.long))\n",
    "\n",
    "    # pad to max_length in batch\n",
    "    source_batch = torch.nn.utils.rnn.pad_sequence(source_batch, padding_value=PAD_TOKEN_IDX, batch_first=True)\n",
    "    target_batch = torch.nn.utils.rnn.pad_sequence(target_batch, padding_value=PAD_TOKEN_IDX, batch_first=True)\n",
    "    return source_batch, target_batch\n",
    "\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_seq2seq_batch)\n",
    "valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_seq2seq_batch)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_seq2seq_batch)\n",
    "\n",
    "for X, y in test_loader:\n",
    "    print(\"Mini-batch X shape:\", X.shape)\n",
    "    print(\"Mini-batch y shape:\", y.shape)\n",
    "    print(\"\\n1st source text:\", X[0])\n",
    "    print(\"1st target text:\", y[0])\n",
    "    break\n",
    "\n",
    "print(\"\\n<sos> token index:\", SOS_TOKEN_IDX)\n",
    "print(\"<eos> token index:\", EOS_TOKEN_IDX)\n",
    "print(\"<pad> token index:\", PAD_TOKEN_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Attention-based Seq2Seq Model\n",
    "\n",
    "### Encoder\n",
    "\n",
    "인코더(Encoder)는 번역할 문장(source sentence)을 입력 받아 순환 신경망(RNN, LSTM, GRU)을 통해 각 token을 순차적으로 인코딩합니다.\n",
    "\n",
    "입력 시퀀스 $X = (x_1, x_2, \\dots, x_{T_x})$가 주어졌을때, Encoder GRU는 각 time-step $t$에서 아래 수식에 따라 hidden state를 업데이트합니다.\n",
    "\n",
    "$$\n",
    "\\mathbf{h}_t = \\text{GRU}_{\\text{enc}}(x_t, \\mathbf{h}_{t-1})\n",
    "$$\n",
    "\n",
    " - $\\mathbf{h}_t$는 현재까지 입력된 토큰들 $x_{1:t}$에 대한 정보를 압축하여 담고 있는 상태 벡터입니다.\n",
    "\n",
    "<mark>실습</mark> `Encoder` 모듈을 완성하세요\n",
    "1. Embedding layer: `self.embedding`레이어를 이용하여 입력 토큰 인덱스를 임베딩 벡터로 변환합니다.\n",
    "2. Droptout layer: 과적합 방지를 위해 임베딩 벡터에 `self.dropout`레이어를 적용합니다.\n",
    "3. GRU layer: 드롭아웃이 적용된 임베딩 벡터를 `self.gru`레이어에 입력합니다.\n",
    "   - GRU 레이어는 다음과 같은 두 가지 텐서를 반환합니다 ([docs](https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html)):\n",
    "     - `output` : 마지막 GRU layer의 <b>모든 time-step</b>에서의 hidden state $(h_1, h_2, \\dots, h_{T_x})$\n",
    "       - shape = `(batch_size, src_len, hidden_dim)`\n",
    "     - `hidden` : 모든 GRU layer의 <b>final hidden state</b> $h_{T_x}$.\n",
    "       - shape = `(num_layers, batch_size, hidden_dim)`\n",
    "\n",
    "4. 최종적으로 인코더는 `output`과 `hidden`을 반환합니다.\n",
    "   - `output`(모든 time-step에서의 hidden state) : 어텐션(attention)의 입력으로 사용됩니다.\n",
    "   - `hidden`(final hidden state) : 디코더의 초기 hidden state로 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_token_index, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx = pad_token_index)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers = num_layers,\n",
    "                          batch_first = True, dropout = dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor of shape (batch_size, src_len) containing token indices for the source sentence.\n",
    "        Returns:\n",
    "            output: Tensor of shape (batch_size, src_len, hidden_dim)\n",
    "                    containing the hidden states from the last GRU layer for all time steps.\n",
    "            hidden: Tensor of shape (num_layers, batch_size, hidden_dim)\n",
    "                    containing the final hidden states of all GRU layers.\n",
    "\n",
    "        \"\"\"\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        return output, hidden  # `output` for attention, `hidden` for decoder hidden state initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size = len(source_vocab), embed_dim = 128, hidden_dim = 256,\n",
    "                  num_layers = 2, pad_token_index = PAD_TOKEN_IDX, dropout = 0.5)\n",
    "\n",
    "X = torch.randint(0, 100, (8, 20))  # (batch_size, src_len)\n",
    "output, hidden = encoder(X)\n",
    "\n",
    "print(\"Input shape:\", X.shape)                          # (batch_size, src_len)\n",
    "print(\"Encoder output shape:\", output.shape)            # (batch_size, src_len, hidden_dim)\n",
    "print(\"Encoder hidden state shape:\", hidden.shape)      # (num_layers, batch_size, hidden_dim)\n",
    "\n",
    "assert output.shape == (8, 20, 256), f\"Expected output shape (8, 20, 256), but got {hidden.shape}\"\n",
    "assert hidden.shape == (2, 8, 256), f\"Expected hidden shape (2, 8, 256), but got {hidden.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot-product similarity\n",
    "두 벡터간의 dot-product similarity를 계산하기 위해서는 `torch.dot`연산을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "vec1 = torch.rand(size = (4,))  # shape: (hidden_dim,)\n",
    "vec2 = torch.rand(size = (4,))  # shape: (hidden_dim,)\n",
    "\n",
    "similarity = torch.dot(vec1, vec2)\n",
    "\n",
    "print(similarity.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벡터(하나의 token)와 행렬(sequence of token)간의 dot-product similarity를 계산하기 위해서는 for loop를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "query = torch.rand(size = (4,))   # shape: (hidden_dim,)\n",
    "keys = torch.rand(size = (3, 4))  # shape: (seq_len, hidden_dim)\n",
    "\n",
    "scores = torch.empty(size = (keys.shape[0],))\n",
    "for i in range(keys.shape[0]):\n",
    "    scores[i] = (torch.dot(query, keys[i]))\n",
    "    \n",
    "print(scores)  # shape: (seq_len,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 반복문은 행렬 곱으로 한 번에 처리할 수 있습니다 (vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# (seq_len, hidden_dim) @ (hidden_dim,) = (seq_len,)\n",
    "scores_vec = keys @ query # (3, 4) @ (4,) -> (3,)\n",
    "\n",
    "print(scores_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mini-batch로 구성된 벡터와 행렬 간의 dot-product 연산을 수행하기 위해서는 `torch.bmm`([docs](https://docs.pytorch.org/docs/stable/generated/torch.bmm.html)) 함수를 사용합니다.\n",
    "- `torch.bmm`은 batch matrix multiplication의 약자로 배치간의 행렬 곱을 수행합니다.\n",
    "- `torch.bmm`의 입력은 반드시 3차원 텐서 `(b, n, m)` 형태여야 하므로, `unsqueeze` ([docs](https://docs.pytorch.org/docs/stable/generated/torch.unsqueeze.html))와 `squeeze` ([docs](https://docs.pytorch.org/docs/stable/generated/torch.squeeze.html))함수를 사용하여 차원을 적절히 맞춰줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "query = torch.randn(2, 4)     # (batch_size, hidden_dim)\n",
    "keys = torch.randn(2, 3, 4)   # (batch_size, seq_len, hidden_dim)\n",
    "\n",
    "# query를 (batch_size, hidden_dim) → (batch_size, hidden_dim, 1)로 변환\n",
    "query = query.unsqueeze(2)    # (2, 4, 1)\n",
    "\n",
    "# (batch_size, seq_len, hidden_dim) @ (batch_size, hidden_dim, 1) = (batch_size, seq_len, 1)\n",
    "scores_bmm = torch.bmm(keys, query)   # (2, 3, 4) @ (2, 4, 1) = (2, 3, 1)\n",
    "scores_bmm = scores_bmm.squeeze(2)    # (2, 3)\n",
    "\n",
    "print(scores_bmm.shape)  # (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot-product Attention\n",
    "\n",
    "Attention mechanism은 디코더가 번역 과정의 각 시점에서 입력 시퀀스의 어느 부분에 주목(attention)해야하는지를 학습하는 방법입니다.\n",
    "\n",
    "1. 인코더(Encoder)는 입력 시퀀스 $X = (x_1, x_2, \\dots, x_{T_x})$ 에 대해 다음과 같은 hidden states를 생성합니다:\n",
    "\n",
    "$$\\mathbf{h}_1, \\mathbf{h}_2, \\dots, \\mathbf{h}_{T_x} \\in \\mathbb{R}^{d_h}$$\n",
    "\n",
    "2. 디코더(decoder) hidden state update:\n",
    "   - time-step $t$에서 디코더는 이전 상태 $\\mathbf{s}_{t-1}$와 이전 출력 토큰 $\\mathbf{y}_{t-1}$을 기반으로 현재 상태를 업데이트합니다.\n",
    "$$\\mathbf{s}_t = \\text{GRU}_{dec}(\\mathbf{y}_{t-1}, \\mathbf{s}_{t-1})$$\n",
    "\n",
    "3. Attention score 계산:\n",
    "   - 디코더 hidden state $\\mathbf{s}_t$와 각 인코더 hidden state $\\mathbf{h}_i$와 사이의 유사도를 계산합니다:\n",
    "\n",
    "    $$\n",
    "    e_{t,i} = f_{attn}(\\mathbf{s}_t, \\mathbf{h}_i) \\in \\mathbb{R}, \\quad \\mathbf{e}_t = [e_{t,1}, e_{t,2}, \\dots, e_{t,T_x}] \\in \\mathbb{R}^{T_x}\n",
    "    $$\n",
    "\n",
    "   - 이번 실습에서는 Weighted dot-product attention를 사용하여 어텐션 스코어를 계산합니다.\n",
    "\n",
    "        $$e_{t,i} = f_{attn}(\\mathbf{s}_t, \\mathbf{h}_i) = \\mathbf{s}_t^\\top \\mathbf{W}_K \\mathbf{h}_i$$\n",
    "\n",
    "       - 여기서 $\\mathbf{W}_K$는 학습가능한 가중치 행렬입니다.\n",
    "       - 내적(dot product)은 두 벡터가 같은 방향일수록 큰 값을 가지므로, 두 벡터간 유사성을 측정하는 자연스러운 방법입니다.\n",
    "\n",
    "\n",
    "4. Attention weights (Softmax normalization)\n",
    "   - 스코어 벡터 $\\mathbf{e}_t$를 소프트맥스로 정규화하여 확률 분포 형태의 어텐션 가중치를 얻습니다:\n",
    "\n",
    "    $$\n",
    "    \\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_{j=1}^{T_x} \\exp(e_{t,j})} \\in \\mathbb{R},\n",
    "    \\quad\n",
    "    \\boldsymbol{\\alpha}_t = \\text{softmax}(\\mathbf{e}_t) \\in [0,1]^{T_x}\n",
    "    $$\n",
    "\n",
    "    $$\\sum_{i=1}^{T} \\alpha_{t,i} = 1$$\n",
    "\n",
    "5. Context Vector (Attention output) 계산\n",
    "    - Attention weights를 이용하여 인코더 hidden state들의 가중합(weighted sum)을 계산합니다\n",
    "\n",
    "        $$ \\mathbf{c}_t = \\sum_{i=1}^{T_x} \\alpha_{t,i} \\mathbf{h}_i \\in \\mathbb{R}^{d_h}$$\n",
    "\n",
    "        - 계산된 Context vector $\\mathbf{c}_t$는 현재 번역 시점에서 필요한 입력 문장의 핵심 정보들을 담고있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "<mark>실습</mark> `DotProductAttention` 모듈을 완성하세요\n",
    "1. `query`: Decoder hidden state 중 마지막 GRU 레이어의 hidden state를 query vector로 사용합니다.\n",
    "   - Shape: `(batch_size, hidden_dim)`\n",
    "2. `keys` : `encoder_outputs`에 선형 변환 (`self.key_projection`)을 적용하여 key vector들을 얻습니다.\n",
    "   - Shape: `(batch_size, src_len, hidden_dim)`\n",
    "3. `values` : `encoder_outputs`을 그대로 value vector로 사용합니다\n",
    "   - Shape: `(batch_size, src_len, hidden_dim)`\n",
    "4. `attention_scores` : `query`와 `keys`의 내적을 구한 뒤, `self.scale`를 곱해줍니다.\n",
    "   - `torch.bmm`, `unsqueeze`, `squeeze`를 이용하세요.\n",
    "   - `self.scale`를 곱하여 softmax를 적용하기 전 텐서의 스케일을 조정해줍니다.\n",
    "   - 결과 텐서 Shape: `(batch_size, src_len)`\n",
    "5. `attention_weights` : `attention_scores`에 `torch.softmax`를 적용합니다.\n",
    "6. `context_vector` : 계산된 `attention_weights`를 이용해 `values`의 weighted sum을 계산합니다\n",
    "   - `torch.bmm`, `unsqueeze`, `squeeze`를 이용하세요.\n",
    "   - 결과 텐서 Shape: `(batch_size, hidden_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.key_projection = nn.Linear(hidden_dim, hidden_dim, bias = False)\n",
    "        self.scale = 1.0 / math.sqrt(hidden_dim)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            decoder_hidden: Tensor of shape (num_layers, batch_size, hidden_dim)\n",
    "                            containing the decoder's current hidden state.\n",
    "            encoder_outputs: Tensor of shape (batch_size, src_len, hidden_dim)\n",
    "                             containing the encoder's outputs for all time step.\n",
    "        Returns:\n",
    "            context_vector: Tensor of shape (batch_size, hidden_dim)\n",
    "                            containing weighted sum of encoder_outputs.\n",
    "            attention_weights: Tensor of shape (batch_size, src_len)\n",
    "                               containing attention weights over source tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        query = ...  # TODO. Shape: (batch_size, hidden_dim)\n",
    "        keys = ... # TODO. Shape: (batch_size, src_len, hidden_dim)\n",
    "        values = ... # TODO. Shape: (batch_size, src_len, hidden_dim)\n",
    "          \n",
    "        attention_scores = ... # TODO. Shape: (batch_size, src_len)\n",
    "        attention_weights = ... # TODO. Shape: (batch_size, src_len)\n",
    "\n",
    "        context_vector = ... # TODO. Shape: (batch_size, hidden_dim)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "attention_layer = DotProductAttention(hidden_dim = 256)\n",
    "decoder_hidden = torch.randn(2, 8, 256)      # (num_layers, batch_size, hidden_dim)\n",
    "encoder_outputs = torch.randn(8, 20, 256)    # (batch_size, src_len, hidden_dim)\n",
    "context_vector, attention_weights = attention_layer(decoder_hidden, encoder_outputs)\n",
    "\n",
    "print(\"context_vector shape:\", context_vector.shape)        # (batch_size, hidden_dim)\n",
    "print(\"attention_weights shape:\", attention_weights.shape)  # (batch_size, src_len)\n",
    "assert context_vector.shape == (8, 256), f\"Expected context_vector shape (8, 256), but got {context_vector.shape}\"\n",
    "assert attention_weights.shape == (8, 20), f\"Expected attention_weights shape (8, 20), but got {attention_weights.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "디코더(Decoder)에서는 어텐션(attention)을 활용하여 인코더 출력으로 부터 `context vector`를 계산하고, 타겟 문장을 한 단어씩 순차적으로 생성합니다\n",
    "\n",
    "<mark>실습</mark> `Decoder` 모듈을 완성하세요\n",
    "\n",
    " - `Decoder`는 한번에 한 time-step의 디코딩만 수행합니다. 즉 입력 토큰의 길이(sequence length)는 항상 `1`입니다. \n",
    " - `seq_length = 1`이므로, 텐서의 차원 처리에 유의하세요.\n",
    "   - 필요에 따라 [`unsqueeze`](https://docs.pytorch.org/docs/main/generated/torch.unsqueeze.html)와 [`squeeze`](https://docs.pytorch.org/docs/main/generated/torch.squeeze.html)메서드를 활용하여 텐서의 차원을 적절히 변환하세요.\n",
    "\n",
    "계산 과정\n",
    "1. Embedding & Dropout: 입력 토큰 인덱스를 `self.embedding`레이어를 통해 임베딩 벡터로 변환합니다. 과적합 방지를 위해 임베딩 벡터에 `self.dropout`를 적용합니다.\n",
    "2. Decoder GRU: dropout을 통과한 임베딩 벡터와 이전 hidden states $\\mathbf{s}_{t-1}$를 `self.gru`에 입력하여 새로운 hidden state $\\mathbf{s}_{t}$를 얻습니다.\n",
    "3. Attention : `self.attention`를 이용하여 `context_vector` $\\mathbf{c}_t$와 `attention_weights` $\\mathbf{\\alpha}_t$를 계산합니다.\n",
    "4. fully-connected layer: `self.fc_out`(`nn.Linear`) 레이어를 이용하여 다음 단어의 예측값(`logits`)을 계산합니다.\n",
    "   - $[\\mathbf{s}_t; \\mathbf{c}_t]$ ($[;]$는 concatenation)를 `self.fc_out`의 입력으로 전달합니다.\n",
    "     - 마지막 GRU layer의 hidden state를 $\\mathbf{s}_t$로 사용하세요.\n",
    "   - `torch.cat`([docs](https://docs.pytorch.org/docs/stable/generated/torch.cat.html))을 이용하여 두 텐서를 concat합니다.\n",
    "   - <mark>주의</mark> concat 순서는 일반적으로 어떤 순서로 하여도 무방하나 이번 실습에서는 채점을 위해 $[\\mathbf{s}_t; \\mathbf{c}_t]$의 <b>순서를 반드시 지켜주세요</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, pad_token_index, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx = pad_token_index)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers = num_layers,\n",
    "                          batch_first = True, dropout = dropout)\n",
    "        \n",
    "        self.attention = DotProductAttention(hidden_dim)\n",
    "\n",
    "        self.fc_out = ... # TODO\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_token, prev_hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_token: Tensor of shape (batch_size,) containing token index for the current time step.\n",
    "            prev_hidden: Tensor of shape (num_layers, batch_size, hidden_dim)\n",
    "                         containing the previous hidden state of the decoder (or encoder's final hidden state).\n",
    "            encoder_outputs: Tensor of shape (batch_size, src_len, hidden_dim)\n",
    "                             containing the encoder outputs for attention.\n",
    "        Returns:\n",
    "            logits: Tensor of shape (batch_size, target_vocab_size) containing prediction scores for the next token.\n",
    "            decoder_hidden : Tensor of shape (num_layers, batch_size, hidden_dim)\n",
    "                             containing the updated decoder hidden state.\n",
    "            attention_weights : Tensor of shape (batch_size, src_len) containing attention weights.\n",
    "        \"\"\"\n",
    "\n",
    "        ## 1. Embed input token\n",
    "        input_token = input_token.unsqueeze(1)  # Shape : (batch_size, 1)\n",
    "        embedded_token = self.dropout(self.embedding(input_token))  # Shape : (batch_size, 1, embed_dim)\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "        ## 2. Update decoder hidden state\n",
    "        \n",
    "        \n",
    "        ## 3. Compute attention weights and context vector\n",
    "        \n",
    "\n",
    "        ## 4. Concatenate decoder hidden state and context vector, then generate output logits\n",
    "\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        return logits, decoder_hidden, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "dec = Decoder(vocab_size = len(target_vocab), embed_dim = 128, hidden_dim = 256,\n",
    "              num_layers = 2, pad_token_index = PAD_TOKEN_IDX, dropout = 0.5)\n",
    "\n",
    "input_token = torch.randint(0, 100, (8,))  # (batch_size,)\n",
    "prev_hidden = torch.randn(2, 8, 256)       # (num_layers, batch_size, hidden_dim)\n",
    "encoder_outputs = torch.randn(8, 20, 256)  # (batch_size, src_len, hidden_dim)\n",
    "\n",
    "logits, decoder_hidden, attention_weights = dec(input_token, prev_hidden, encoder_outputs)\n",
    "\n",
    "print(\"Decoder logits shape:\", logits.shape)               # (batch_size, target_vocab_size)\n",
    "print(\"Decoder hidden state shape:\", decoder_hidden.shape) # (num_layers, batch_size, hidden_dim)\n",
    "print(\"attention_weights shape:\", attention_weights.shape) # (batch_size, src_len)\n",
    "\n",
    "assert logits.shape == (8, len(target_vocab)), f\"Expected logits shape (8, {len(target_vocab)}), but got {logits.shape}\"\n",
    "assert hidden.shape == (2, 8, 256), f\"Expected hidden shape (2, 8, 256), but got {hidden.shape}\"\n",
    "assert attention_weights.shape == (8, 20), f\"Expected attention_weights shape (2, 20), but got {attention_weights.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq\n",
    "\n",
    "앞서 구현한 `Encoder`와 `Decoder`를 결합하여 `Seq2Seq` 모델을 완성합니다.\n",
    "\n",
    "`Seq2Seq` 모델 구조는 지난 실습과 동일합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.target_vocab_size = self.decoder.embedding.num_embeddings\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_prob = 0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor of shape (batch_size, source_seq_len) containing token indices for the source sentences.\n",
    "            tgt: Tensor of shape (batch_size, target_seq_len) containing token indices for the target sentences.\n",
    "            teacher_forcing_prob: Probability of using the ground-truth token as the next input.\n",
    "\n",
    "        Returns:\n",
    "            output_logits: Tensor of shape (batch_size, target_seq_len, target_vocab_size)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, target_seq_len = tgt.size()\n",
    "        output_logits = torch.zeros(batch_size, target_seq_len, self.target_vocab_size, device=tgt.device)\n",
    "\n",
    "        # Encode source sequence\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        # First decoder input is <sos>\n",
    "        current_input_token = tgt[:,0]  # (batch_size,)\n",
    "\n",
    "        # Decode one step at at time\n",
    "        for t in range(1, target_seq_len):\n",
    "            decoder_logits, hidden, _ = self.decoder(current_input_token, hidden, encoder_outputs)\n",
    "            output_logits[:, t] = decoder_logits\n",
    "\n",
    "            # choose next input token: teacher forcing or model’s own prediction\n",
    "            use_teacher_forcing = torch.rand(1).item() < teacher_forcing_prob\n",
    "            current_input_token = tgt[:, t] if use_teacher_forcing else decoder_logits.argmax(1)\n",
    "        return output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "hidden_dim = 512\n",
    "num_layers = 2\n",
    "\n",
    "enc = Encoder(vocab_size = len(source_vocab), embed_dim = embed_dim, hidden_dim = hidden_dim,\n",
    "              num_layers = num_layers, pad_token_index = PAD_TOKEN_IDX, dropout = 0.5)\n",
    "\n",
    "dec = Decoder(vocab_size = len(target_vocab), embed_dim = embed_dim, hidden_dim = hidden_dim,\n",
    "              num_layers = num_layers, pad_token_index = PAD_TOKEN_IDX, dropout = 0.5)\n",
    "\n",
    "model = Seq2Seq(enc, dec)\n",
    "\n",
    "\n",
    "X = torch.randint(0, 100, (32, 20)) # (batch_size, src_len)\n",
    "y = torch.randint(0, 100, (32, 30)) # (batch_size, tgt_len)\n",
    "\n",
    "logits = model(X, y, teacher_forcing_prob=0.5)\n",
    "print(\"Seq2Seq logits shape:\", logits.shape)  # (batch_size, tgt_len, target_vocab_size)\n",
    "\n",
    "assert logits.shape == (32, 30, len(target_vocab)), f\"Expected logits shape (32, 30, {len(target_vocab)}), but got {logits.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>실습</mark> 완성한 Attention-based Seq2Seq 번역 모델을 학습해보세요.\n",
    " - 학습에는 <u>약 1~2분</u> 정도 소요됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq():\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    SOURCE_VOCAB_SIZE   = len(source_vocab)\n",
    "    TARGET_VOCAB_SIZE  = len(target_vocab)\n",
    "    embed_dim       = 256\n",
    "    hidden_dim      = 512\n",
    "    num_layers      = 1\n",
    "    encoder_dropout = 0.1\n",
    "    decoder_dropout = 0.1\n",
    "    grad_clip        = 1.0\n",
    "\n",
    "    num_epochs = 3\n",
    "    batch_size = 64\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                              shuffle=True, num_workers= 4, collate_fn=collate_seq2seq_batch)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, \n",
    "                              shuffle=False, num_workers= 4, collate_fn=collate_seq2seq_batch)\n",
    "    test_dataloader  = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                              shuffle=False, num_workers= 4, collate_fn=collate_seq2seq_batch)\n",
    "\n",
    "    enc = Encoder(vocab_size = SOURCE_VOCAB_SIZE, embed_dim = embed_dim, hidden_dim = hidden_dim, \n",
    "                  num_layers = num_layers, pad_token_index = PAD_TOKEN_IDX, dropout = encoder_dropout)\n",
    "    dec = Decoder(vocab_size = TARGET_VOCAB_SIZE, embed_dim = embed_dim, hidden_dim = hidden_dim, \n",
    "                  num_layers = num_layers, pad_token_index = PAD_TOKEN_IDX, dropout = decoder_dropout)\n",
    "    model = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "\n",
    "    print(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable parameters')\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index = PAD_TOKEN_IDX)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, device, train_dataloader, criterion, optimizer, epoch, grad_clip)\n",
    "        val_loss = evaluate_one_epoch(model, device, val_dataloader, criterion, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1:02} | Train Loss: {train_loss:.3f} | Val. Loss: {val_loss:.3f}\")\n",
    "        print(f'\\t | Train PPL: {math.exp(train_loss):7.3f} | Val. PPL: {math.exp(val_loss):7.3f}')\n",
    "\n",
    "    test_loss = evaluate_one_epoch(model, device, test_dataloader, criterion)\n",
    "    print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "model = train_seq2seq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 구현이 잘 되었다면 별도의 하이퍼파라미터 튜닝(hyperparameter tuning)없이 `Validation PPL < 8.2`를 달성하실 수 있습니다\n",
    "\n",
    "---\n",
    "\n",
    "## Attention Visualization\n",
    "어텐션(attention) 메커니즘의 가장 큰 장점 중 하나는 출력을 생성할 때 입력 문장의 어떤 부분을 참고했는지 시각화할 수 있다는 점입니다.\n",
    "\n",
    "이는 단순히 모델의 성능을 개선하는 것을 넘어, <b>모델의 해석 가능성(interpretability)</b>을 크게 향상시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, max_len = 50):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    tokens = [SOS_TOKEN] + tokenizer_french(sentence) + [EOS_TOKEN]\n",
    "    source_indices = torch.tensor(source_vocab(tokens), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_outputs, hidden = model.encoder(source_indices)\n",
    "\n",
    "        input_tok = torch.tensor([SOS_TOKEN_IDX], device=device)\n",
    "        result_tokens = []\n",
    "\n",
    "        assert enc_outputs.shape[1] == len(tokens)\n",
    "\n",
    "        attentions = torch.zeros(max_len, len(tokens))\n",
    "        for i in range(max_len):\n",
    "            pred, hidden, attn = model.decoder(input_tok, hidden, enc_outputs)\n",
    "            attentions[i] = attn\n",
    "\n",
    "            next_token_idx = pred.argmax(1).item()\n",
    "            result_tokens.append(target_vocab.lookup_token(next_token_idx))\n",
    "            input_tok = torch.tensor([next_token_idx], device=device)\n",
    "\n",
    "            if next_token_idx == EOS_TOKEN_IDX:\n",
    "                break\n",
    "    return tokens, result_tokens, attentions[:len(result_tokens)]\n",
    "\n",
    "# Quick sanity check\n",
    "example_fr = \"Je suis désolé si je vous ai embarrassés.\"\n",
    "print(f\"FR ➡️ EN  : {example_fr}\")\n",
    "\n",
    "src_tokens, translation, attention = translate_sentence(example_fr, model)\n",
    "print(f\"Predicted: {translation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot_attention(sentence, translation, attention):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    cax = ax.matshow(attention, cmap=\"bone\")\n",
    "    # fig.colorbar(cax)\n",
    "\n",
    "    ax.set_xticks(ticks=np.arange(len(sentence)), labels=sentence, rotation=90, size=15)\n",
    "    ax.set_yticks(ticks=np.arange(len(translation)), labels=translation, size=15)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_attention(src_tokens, translation, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 어텐션 시각화를 살펴보면 각 출력 토큰을 생성할때 입력의 적절한 부분에 주의를 집중하고 있다는 점을 확인할 수 있습니다\n",
    " - `Je` <-> `I`\n",
    " - `désolé` <-> `sorry`\n",
    " - `si` <-> `if`\n",
    " - `embarrassés` <-> `embarassed`\n",
    " - `.` <-> `<eos>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
