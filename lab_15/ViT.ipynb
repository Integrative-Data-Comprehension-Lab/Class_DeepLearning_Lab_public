{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT)\n",
    "이번 실습에서는 Vision Transformer (ViT) 아키텍처를 직접 구현해보겠습니다. \n",
    "\n",
    "ViT는 자연어 처리(NLP)에서 주로 사용되던 <b>Transformer 아키텍처</b>를 이미지에 적용하여 이미지 분류 문제에서 뛰어난 성능을 발휘하는 모델입니다. \n",
    "\n",
    "이번 실습의 목표는 ViT를 구성하는 <b>핵심 모듈들을 단계별로 구현</b>해봄으로써, ViT 아키텍처의 구조와 동작 원리를 자세히 이해하는 것입니다.\n",
    "\n",
    "ViT는 다음과 같은 모듈들로 구성되어 있습니다\n",
    "- `ImagePatchifier` : 입력 이미지를 여러 개의 작은 패치(patch)로 분할합니다.\n",
    "- `PatchEmbedding` : 분할된 이미지 패치를 벡터 형태의 임베딩으로 변환합니다. ViT에서는 각 이미지 패치가 문장의 단어(token)처럼 취급됩니다.\n",
    "- `MultiHeadSelfAttention` : 입력 이미지의 여러 패치(patch)들 간의 Multi-head Self-attention을 계산합니다. 이를 통해 각 패치는 이미지 전체 맥락(context)을 반영한 더욱 풍부하고 정교한 표현(representation)을 학습합니다.\n",
    "- `FeedForwardNetwork` : Self-Attention의 출력을 비선형 변환하여 더욱 복잡하고 풍부한 특징을 학습하게 합니다.\n",
    "- `TransformerEncoderLayer` : `MultiHeadSelfAttention`과 `FeedForwardNetwork`를 결합한 하나의 Transformer 인코더 블록입니다.\n",
    "- `ViT` : 위의 모든 모듈들을 종합하여 Vision Transformer 모델을 구현합니다.\n",
    "\n",
    "Original paper : An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale ([link](https://arxiv.org/pdf/2010.11929))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from training_utilities import load_cifar10_dataloaders, train_one_epoch, evaluate_one_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Self-Attention\n",
    "\n",
    "Transformer의 핵심 모듈로, 입력 토큰(token)들 간의 상호작용을 학습합니다. 이를 통해 각 토큰은 나머지 전체 토큰의 맥락(context)을 반영한 더욱 풍부하고 정교한 표현(representation)을 얻게 됩니다.\n",
    "\n",
    "\n",
    "### Scaled Dot-Product Self-Attention\n",
    "단일 head의 Self-Attention은 다음 수식을 통해 계산됩니다:\n",
    "\n",
    "1. 입력 토큰 임베딩\n",
    "   $$\\mathbf{X} = (\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_T) \\in \\mathbb{R}^{T \\times d_{embed}}$$\n",
    "2. 입력 임베딩을 선형변환하여 Query, Key, Value를 얻습니다.\n",
    "   $$\\mathbf{Q} = \\mathbf{X} \\mathbf{W}_Q \\in \\mathbb{R}^{T \\times d_{k}},\\quad\n",
    "     \\mathbf{K} = \\mathbf{X} \\mathbf{W}_K \\in \\mathbb{R}^{T \\times d_{k}},\\quad\n",
    "     \\mathbf{V} = \\mathbf{X} \\mathbf{W}_V \\in \\mathbb{R}^{T \\times d_{v}}$$\n",
    "   -  $\\mathbf{W}_Q,\\mathbf{W}_K \\in \\mathbb{R}^{d_{embed} \\times d_{k}}, \\mathbf{W}_V \\in \\mathbb{R}^{d_{embed} \\times d_{v}}$ 는 학습 가능한 가중치 행렬입니다.\n",
    "3. <b>Scaled Dot-product Self-attention</b>\n",
    "   $$ \\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}})\\mathbf{V} \\in \\mathbb{R}^{T \\times d_{v}}$$\n",
    "   - 여기서 $d_k$는 key벡터의 차원의 크기입니다.\n",
    "\n",
    "### Multi-head Self-Attention\n",
    "Self-Attention을 여러 번 병렬로 수행하면, 서로 다른 관점에서의 다양한 관계를 학습할 수 있습니다. 이를 <b>Multi-head attention</b>이라고 합니다.\n",
    "\n",
    "$$ \\text{head}_i = \\text{Attention}(\\mathbf{X} \\mathbf{W}_Q^{(i)}, \\mathbf{X} \\mathbf{W}_K^{(i)}, \\mathbf{X} \\mathbf{W}_V^{(i)}) \\in \\mathbb{R}^{T \\times d_{v}}$$\n",
    "$$ \\text{MultiHeadSelfAttention}(\\mathbf{X}) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)\\mathbf{W}_O \\in \\mathbb{R}^{T \\times d_{embed}}$$\n",
    " - $h$는 head의 수\n",
    " - $\\mathbf{W}_Q^{(i)},\\mathbf{W}_K^{(i)} \\in \\mathbb{R}^{d_{embed} \\times d_{k}}, \\mathbf{W}_V^{(i)} \\in \\mathbb{R}^{d_{embed} \\times d_{v}}$ : head $i$의 선형 변환 행렬\n",
    " - $\\mathbf{W}_O \\in \\mathbb{R}^{h \\cdot d_v \\times d_{embed}}$ is output linear projection matrix \n",
    "\n",
    "일반적으로 다음과 같은 관계를 만족하도록 모델을 구성합니다.\n",
    " - $d_k = d_v$\n",
    " - $d_{embed} = h \\times d_k$\n",
    "\n",
    "---\n",
    "<mark>실습</mark> `MultiHeadSelfAttention`을 완성하세요.\n",
    "\n",
    "1. Query, Key, Value projection: 입력을 선형변환하여 `Q`, `K`, `V`를 얻습니다.\n",
    "\n",
    "2. multiple heads로 분할: `Q`, `K`, `V`를 `(batch_size, num_heads, seq_length, head_dim)`의 shape을 가지도록 변환합니다.\n",
    "   - `embed_dim = head_dim * num_heads`의 관계를 이용하여, 선형변환된 벡터를 head 개수만큼 쪼개어 사용합니다.\n",
    "   - `torch.Tensor.view` ([docs](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html))를 이용하여 텐서를 reshape합니다.\n",
    "   - `torch.permute` ([docs](https://pytorch.org/docs/stable/generated/torch.permute.html))를 이용하여 텐서의 차원 순서를 재배열합니다\n",
    "\n",
    "1. Scaled Dot-Product Attention 계산\n",
    "   - `Q`와 `K`의 scaled-dot dot-product를 계산하여 `attention_scores`를 계산합니다.\n",
    "     - `torch.transpose` ([docs](https://pytorch.org/docs/stable/generated/torch.transpose.html))\n",
    "     - `torch.matmul` ([docs](https://pytorch.org/docs/stable/generated/torch.matmul.html)) 함수는 각 텐서의 마지막 두 차원에 대한 행렬 곱을 수행하고, 앞의 차원은 batch차원으로 간주합니다.\n",
    "     - `** 0.5` 연산을 이용하여 제곱근을 계산하세요\n",
    "   - `torch.softmax` 함수를 이용하여 `attention_weights`를 계산합니다.\n",
    "   - `attention_weights`에 dropout layer를 적용한 뒤, `V`와 weighted sum을 계산합니다. \n",
    "\n",
    "2. Output projection\n",
    "   - 모든 head의 attention출력값을 다시 합쳐 `(batch_size, seq_len, embed_dim)`의 shape을 갖도록 만들어줍니다.\n",
    "     - `torch.Tensor.reshape` ([docs](https://docs.pytorch.org/docs/stable/generated/torch.Tensor.reshape.html)) 함수와 `torch.permute` ([docs](https://pytorch.org/docs/stable/generated/torch.permute.html)) 함수를 이용하세요.\n",
    "   - 그 후, $\\mathbf{W}_O$에 대응하는 선형 변환(`self.out_projection`)을 적용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self Attention Module.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, dropout_prob = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim              # total hidden dimension\n",
    "        self.num_heads = num_heads              # number of attention heads\n",
    "        self.head_dim = embed_dim // num_heads  # dimension per head (d_k)\n",
    "        assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads.\"\n",
    "\n",
    "        self.query_projection = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_projection = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_projection = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.out_projection = ... # TODO\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, seq_length, embed_dim)\n",
    "\n",
    "        Returns:\n",
    "            attention_output: Tensor of shape (batch_size, seq_length, embed_dim).\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, embed_dim = x.size() \n",
    "\n",
    "        ## Project input embeddings into query, key, and value -> obtain (batch_size, seq_len, embed_dim)\n",
    "        Q = self.query_projection(x)\n",
    "        K = self.key_projection(x)\n",
    "        V = self.value_projection(x)\n",
    "\n",
    "        ## Reshape to split into multiple heads -> obtain (batch_size, seq_len, num_heads, head_dim)\n",
    "        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "\n",
    "        ## Permute to bring head dimension forward -> obtain (batch_size, num_heads, seq_len, head_dim)\n",
    "        Q = Q.permute(0, 2, 1, 3)\n",
    "        K = K.permute(0, 2, 1, 3)\n",
    "        V = V.permute(0, 2, 1, 3)\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "        ## Scaled dot-product attention for each head\n",
    "\n",
    "\n",
    "        ## Combine multiple heads and apply output projection\n",
    "\n",
    "\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "embed_dim = 64\n",
    "num_heads = 8\n",
    "seq_length = 10\n",
    "batch_size = 32\n",
    "\n",
    "mha_module = MultiHeadSelfAttention(embed_dim = embed_dim, num_heads = num_heads, dropout_prob = 0.1)\n",
    "\n",
    "X = torch.randn(batch_size, seq_length, embed_dim) # (batch_size, seq_length, embed_dim)\n",
    "attention_output = mha_module(X)\n",
    "\n",
    "print(\"MultiHeadSelfAttention output shape:\", attention_output.shape)\n",
    "\n",
    "assert attention_output.shape == (batch_size, seq_length, embed_dim), f\"Expected output shape: ({batch_size}, {seq_length}, {embed_dim}), but got {attention_output.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Networks\n",
    "\n",
    "`MultiHeadSelfAttention` 모듈의 출력값에 feed-forward network(FFN)를 적용합니다.\n",
    "- 이 네트워크는 <b>각 위치(position)별로 독립적</b>으로 작동하며, 시퀀스의 모든 위치에 <b>동일한 변환</b>을 적용합니다. \n",
    "- FFN은 Attention이 학습한 관계 표현을 더 복잡하게 변환하여, 모델이 비선형적이고 정교한 패턴을 학습할 수 있도록 돕습니다.\n",
    "\n",
    "$$ \\text{FFN}(\\mathbf{x}) = \\text{GELU}(\\mathbf{x}\\mathbf{W}_1+b_1)\\mathbf{W}_2+b_2$$\n",
    "\n",
    "<mark>실습</mark> `FeedForwardNetwork`을 완성하세요.\n",
    "\n",
    "1. linear layer 1: 임베딩 차원(`hidden_dim`)을 더 큰 차원(`feedforward_dim`)으로 확장합니다.\n",
    "2. GELU activation: `torch.nn.GELU` ([docs](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html))를 사용하세요\n",
    "3. dropout layer: 정규화를 위해 `nn.Dropout`을 적용합니다.\n",
    "4. linear layer 2: 확장된 차원을 다시 원래 임베딩 차원으로 축소합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Networks\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, feedforward_dim, dropout_prob = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, feedforward_dim),\n",
    "            ##### YOUR CODE START #####\n",
    "\n",
    "\n",
    "            ##### YOUR CODE END #####\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : Tensor of shape (batch_size, seq_length, hidden_dim)\n",
    "\n",
    "        Returns:\n",
    "            out: Tensor of shape (batch_size, seq_length, hidden_dim)\n",
    "        \"\"\"\n",
    "        out = self.ffn(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "embed_dim = 64\n",
    "feedforward_dim = 1024\n",
    "seq_length = 10       # Sequence length\n",
    "batch_size = 32       # Batch size\n",
    "\n",
    "ffn = FeedForwardNetwork(hidden_dim = embed_dim, feedforward_dim = feedforward_dim, dropout_prob = 0.2)\n",
    "\n",
    "X = torch.randn(batch_size, seq_length, embed_dim) # (batch_size, seq_length, embed_dim)\n",
    "ffn_output = ffn(X)\n",
    "\n",
    "print(\"FeedForwardNetwork output shape:\", ffn_output.shape)\n",
    "\n",
    "assert ffn_output.shape == (batch_size, seq_length, embed_dim), f\"Expected output shape: ({batch_size}, {seq_length}, {embed_dim}), but got {ffn_output.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerEncoderLayer\n",
    "\n",
    "앞서 구현한 모듈들을 조합하여 Transformer encoder 레이어를 구현합니다.\n",
    "\n",
    "<img src=\"resources/vit_encoder.png\" style=\"width:200px;\">\n",
    "\n",
    "<mark>실습</mark> `TransformerEncoderLayer`을 완성하세요.\n",
    "\n",
    "1. First Sub-layer (Multi-Head Self Attention):\n",
    "    - 입력값에 Layer normalization을 적용합니다: `nn.LayerNorm` ([docs](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html))를 이용하세요\n",
    "    - 그 결과를 `MultiHeadSelfAttention` 모듈에 통과시키고, dropout을 적용합니다.\n",
    "    - residual connection: dropout이 적용된 출력값에 이 sub-layer의 입력값을 더해줍니다.\n",
    "\n",
    "2. Second Sub-layer (Feed-Forward Network):\n",
    "    - 첫번째 sub-layer의 출력값에 Layer normalization을 적용합니다.\n",
    "    - `FeedForwardNetwork` 모듈에 통과시키고, dropout을 적용합니다.\n",
    "    - residual connection: dropout이 적용된 출력값에 이 sub-layer의 입력값을 더해줍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Encoder Layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, feedforward_dim, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mha = MultiHeadSelfAttention(embed_dim = embed_dim, num_heads = num_heads, dropout_prob = dropout_prob)\n",
    "        self.ffn = FeedForwardNetwork(hidden_dim = embed_dim, feedforward_dim = feedforward_dim, dropout_prob = dropout_prob)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_prob)\n",
    "        self.dropout2 = nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : Tensor of shape (batch_size, seq_length, embed_dim)\n",
    "\n",
    "        Returns:\n",
    "            out : Tensor of shape (batch_size, seq_length, embed_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "        ## Multi-Head Self Attention\n",
    "        \n",
    "        \n",
    "        ## Feed Forward Network\n",
    "\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "embed_dim = 64\n",
    "num_heads = 8\n",
    "feedforward_dim = 1024\n",
    "seq_length = 10       # Sequence length\n",
    "batch_size = 32       # Batch size\n",
    "\n",
    "encoder = TransformerEncoderLayer(embed_dim = embed_dim, num_heads = num_heads, \n",
    "                                  feedforward_dim = feedforward_dim, dropout_prob = 0.2)\n",
    "\n",
    "x = torch.randn(batch_size, seq_length, embed_dim) # (batch_size, seq_length, embed_dim)\n",
    "encoder_output = encoder(x)\n",
    "print(\"TransformerEncoderLayer output shape:\", encoder_output.shape)\n",
    "\n",
    "assert encoder_output.shape == (batch_size, seq_length, embed_dim), f\"Expected output shape: ({batch_size}, {seq_length}, {embed_dim}), but got {encoder_output.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ImagePatchifier\n",
    "\n",
    "입력 이미지를 <b>서로 겹치지 않는 패치(patch)</b>들로 분할하고, 이를 Transformer에서 처리할 수 있는 <b>시퀀스(sequence)형태</b>로 변환합니다.\n",
    "\n",
    "- 입력 이미지 차원: $(C, H, W)$\n",
    "- 출력 이미지 차원: $(N, C \\times P^2)$ 형태로 변환됩니다.\n",
    "  - $(H, W)$는 원본 이미지의 높이와 너비 입니다. 편의를 위해 $H = W$라고 가정합니다.\n",
    "  - $C$는 채널 수, $(P, P)$는 패치의 크기입니다.\n",
    "  - $N = H \\times W/P^2$는 생성되는 패치의 개수 입니다.\n",
    "\n",
    "<mark>실습</mark> `ImagePatchifier`을 완성하세요.\n",
    "\n",
    "1. <b>Patch Extraction</b>: 이미지를 `(patch_size, patch_size)` 크기의 패치들로 분할합니다.\n",
    "    - `torch.Tensor.Unfold` ([docs](https://docs.pytorch.org/docs/2.8/generated/torch.Tensor.unfold.html))함수 이용를 이용합니다.\n",
    "    - 입력 shape : `(batch_size, num_channels, height, width)`\n",
    "    - 출력 shape : `(batch_size, num_channels, num_patches_h, num_patches_w, patch_size, patch_size)`\n",
    "2. <b>Flattening</b>: 각 patch들을 1차원 벡터로 펼쳐주고(Flatten), Transformer에 입력할 수 있는 시퀀스 형태로 변환합니다.\n",
    "    - `view`, `reshape`, `permute`함수를 적절히 활용하세요.\n",
    "    - 입력 shape : `(batch_size, num_channels, num_patches_h, num_patches_w, patch_size, patch_size)`\n",
    "    - 출력 shape : `(batch_size, num_patches, num_channels * patch_size * patch_size)`\n",
    "    - 힌트\n",
    "      - 여러 패치들을 시퀀스 형태로 Flatten하는 과정은 `(num_patches_h, num_patches_w)` 형태의 텐서를 `(num_patches_h * num_patches_w)`의 형태로 변환하는 것과 동일합니다.\n",
    "      - 각 패치는 `(num_channels, patch_size, patch_size)`의 shape을 가지는 3차원 텐서이며, 이를 1차원 텐서로 Flatten해줍니다.\n",
    "      - <mark>주의</mark>: 일반적으로 flatten 순서는 상관없이만, 이번 실습에서는 채점을 위해 height → width 순서를 유지해주세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePatchifier(nn.Module):\n",
    "    \"\"\" Split images into non-overlapping patches and flatten each patch. \"\"\"\n",
    "    def __init__(self, image_size, patch_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        assert image_size % patch_size == 0, \"Image size must be divisible by the patch size.\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor of shape (batch_size, num_channels, height, width). Here, height = width = image_size\n",
    "        Returns:\n",
    "            flattened_patches: Tensor of shape (batch_size, num_patches, num_channels * patch_size * patch_size)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, num_channels, _, _ = x.shape\n",
    "\n",
    "        ## Unfold image along height and width to extract non-overlapping patches.\n",
    "        # After two unfolds, (batch_size, num_channels, num_patches_h, num_patches_w, patch_size, patch_size)\n",
    "        patches = x.unfold(dimension = 2, size = self.patch_size, step = self.patch_size) \\\n",
    "                   .unfold(dimension = 3, size = self.patch_size, step = self.patch_size)\n",
    "        \n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "        \n",
    "        return flattened_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "image_size = 224\n",
    "patch_size = 16\n",
    "\n",
    "image_patchfier = ImagePatchifier(image_size = image_size, patch_size = patch_size)\n",
    "X = torch.randn(batch_size, 3, image_size, image_size) # (batch_size, num_channels, height, width)\n",
    "flattened_patches = image_patchfier(X)\n",
    "print(\"ImagePatchifier output shape:\", flattened_patches.shape)\n",
    "\n",
    "assert flattened_patches.shape == (batch_size, (image_size / patch_size) ** 2, 3 * patch_size * patch_size), f\"Expected output shape: ({batch_size}, {(image_size / patch_size) ** 2}, {3 * patch_size * patch_size}), but got {flattened_patches.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ImagePatchifier` 모듈을 잘 구현하셨다면, 아래 코드를 통해 <b>새의 이미지</b>가 패치 단위로 분할된 결과를 시각적으로 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def visualize_patches(image, patch_size):\n",
    "    patchifier = ImagePatchifier(image_size = 224, patch_size = patch_size)\n",
    "\n",
    "    patches = patchifier(image)[0]\n",
    "    num_patches = patches.shape[0]\n",
    "    grid_size = int(np.sqrt(num_patches))\n",
    "\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(8, 8))\n",
    "    for idx, patch in enumerate(patches):\n",
    "        row = idx // grid_size\n",
    "        col = idx % grid_size\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        patch = patch.view(-1, patch_size, patch_size).permute(1, 2, 0).numpy()\n",
    "        ax.imshow(patch)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "img = Image.open('resources/n01580077_1031.JPEG')\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img_tensor = transform(img).unsqueeze(0)  # (1, num_channels, height, width)\n",
    "\n",
    "visualize_patches(image = img_tensor, patch_size = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PatchEmbedding\n",
    "`ImagePatchifier`에서 얻은 시퀀스 형태의 패치들을 임베딩 벡터(embedding vector)로 변환한 후, `[CLS]`토큰과, positional embedding을 추가합니다.\n",
    "\n",
    "<center><img src=\"resources/vit_model.png\" style=\"width:500px;\"></center>\n",
    "\n",
    "1. `ImagePatchifier`의 출력:\n",
    "\n",
    "   $$[\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_N] \\in \\mathbb{R}^{N\\times (P^2 \\cdot C)}$$\n",
    "\n",
    "   - $C$는 채널 수, $(P, P)$는 패치의 크기입니다.\n",
    "   - $N = H \\cdot W/P^2$는 패치의 개수 입니다. ($(H, W)$는 원본 이미지의 높이와 너비)\n",
    "\n",
    "2. Patch embedding\n",
    "\n",
    "   - 각 이미지 패치를 $d_{embed}$차원으로 선형변환합니다.\n",
    "        $$[\\mathbf{x}_1 \\mathbf{E},\\mathbf{x}_2 \\mathbf{E}, ..., \\mathbf{x}_N \\mathbf{E}] \\in \\mathbb{R}^{N\\times d_{embed}}$$\n",
    "\n",
    "        - $\\mathbf{E} \\in \\mathbb{R}^{(P^2 \\cdot C) \\times d_{embed}}$ 는 학습가능한 선형 변환 행렬\n",
    "\n",
    "3. Prepend `[CLS]` token\n",
    "\n",
    "   - `[CLS]`는 이미지 분류를 위한 특별 토큰으로, 이 토큰에 대응하는 학습 가능한 임베딩 벡터 $\\mathbf{x}_{CLS}$를 시퀀스의 맨 앞에 추가합니다.\n",
    "        $$[\\mathbf{x}_{CLS}, \\mathbf{x}_1 \\mathbf{E},\\mathbf{x}_2 \\mathbf{E}, ..., \\mathbf{x}_N \\mathbf{E}] \\in \\mathbb{R}^{(N+1) \\times d_{embed}}$$\n",
    "\n",
    "        - `[CLS]` 토큰에 대응하는 Tranformer encoder의 최종 출력은 이미지(시퀀스) 전체의 표현을 담도록 학습되며, 여기에 MLP head를 연결하여 이미지 분류 작업을 수행합니다\n",
    "\n",
    "4. Position embeddings\n",
    "\n",
    "   - 학습가능한(learnable) Positional Embedding $\\mathbf{E}_{pos}$을 더해줍니다. 이를 통해 모델은 각 패치의 위치를 구분하고, 패치들 간의 공간적 관계를 이해할 수 있습니다.\n",
    "        $$[\\mathbf{x}_{CLS}, \\mathbf{E},\\mathbf{x}_2 \\mathbf{E}, ..., \\mathbf{x}_N \\mathbf{E}] + \\mathbf{E}_{pos}$$\n",
    "        - $\\mathbf{E}_{pos} \\in \\mathbb{R}^{(N+1)\\times d_{embed}}$\n",
    "\n",
    "\n",
    "\n",
    "<mark>실습</mark> 위 설명을 참고하여 `PatchEmbedding`을 완성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Project flattened image patches into an embedding space, prepend a classification token,\n",
    "    and adds learnable positional embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_patches, patch_size, embed_dim, num_channels = 3):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        self.patch_projection = ... # TODO\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.empty(1, 1, embed_dim)) # learnable classification token\n",
    "        self.position_embeddings = nn.Parameter(torch.empty(1, num_patches + 1, embed_dim)) # learnable positional embeddings\n",
    "\n",
    "        self._initialize_parameters()\n",
    "\n",
    "    def _initialize_parameters(self):\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.position_embeddings, std=0.02)\n",
    "\n",
    "    def forward(self, flattened_patches):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            flattened_patches: Tensor of shape (batch_size, num_patches, num_channels * patch_size * patch_size)\n",
    "        Returns:\n",
    "            embeddings: Tensor of shape (batch_size, num_patches + 1, embed_dim)\n",
    "        \"\"\"\n",
    "        batch_size, num_patches, _ = flattened_patches.shape\n",
    "\n",
    "        ## Project patches to embedding dimension\n",
    "        patch_embeddings = self.patch_projection(flattened_patches)  # (batch_size, num_patches, embed_dim)\n",
    "\n",
    "        ## Broadcast [CLS] token to batch size\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # (batch_size, 1, embed_dim)\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "        ## Concatenate class token with patch embeddings\n",
    "\n",
    "\n",
    "        ## Add positional embeddings\n",
    "\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "image_size = 224\n",
    "patch_size = 16\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "embed_dim = 512\n",
    "\n",
    "to_patch_embedding = PatchEmbedding(num_patches = num_patches, patch_size = patch_size, embed_dim = embed_dim)\n",
    "X = torch.randn(batch_size, num_patches, 3 * patch_size * patch_size) # (batch_size, num_patches, num_channels * patch_size * patch_size)\n",
    "\n",
    "patch_embeddings = to_patch_embedding(X)\n",
    "print(\"PatchEmbedding output shape:\", patch_embeddings.shape)\n",
    "\n",
    "assert patch_embeddings.shape == (batch_size, num_patches + 1, embed_dim), f\"Expected output shape: ({batch_size}, {num_patches + 1}, {embed_dim}), but got {patch_embeddings.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT (Vision Transformer)\n",
    "지금까지 구현한 모듈들을 모두 조합하여 ViT 아키텍쳐를 완성합니다.\n",
    "\n",
    "<mark>실습</mark> `ViT`를 완성하세요.\n",
    "1. <b>Patchification</b>: `ImagePatchifier`를 사용해 이미지를 겹치지 않는 패치들로 분할하고, 시퀀스 형태로 변환합니다.\n",
    "2. <b>Patch Embedding</b>: `PatchEmbedding` 모듈을 각 패치를 임베딩 벡터로 변환합니다.\n",
    "3. <b>Transformer Encoder</b>: `TransformerEncoderLayer`를 `num_transformer_layers`만큼 통과시킵니다.\n",
    "4. <b>Classification Head</b>\n",
    "    - `[CLS]` token에 대응되는 Transformer Encoder의 출력값을 추출하여, 이미지 전체를 대표하는 임베딩으로 사용합니다.\n",
    "    - 해당 임베딩에 mlp_head (`nn.Linear`)를 적용하여 최종 분류 결과(logits) 값을 얻습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT) model.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_size, patch_size, num_channels, embed_dim,\n",
    "                 num_transformer_layers, num_heads, feedforward_dim, \n",
    "                 num_classes, dropout_prob=0.1):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.patchifier = ImagePatchifier(image_size = image_size, patch_size = patch_size)\n",
    "        self.to_patch_embedding = PatchEmbedding(self.patchifier.num_patches, patch_size, embed_dim, num_channels)\n",
    "\n",
    "        self.transformer_encoder = nn.ModuleList([\n",
    "            TransformerEncoderLayer(embed_dim, num_heads, feedforward_dim, dropout_prob)\n",
    "            for _ in range(num_transformer_layers)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.mlp_head = ... # TODO\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Tensor of shape (batch_size, num_channels, height, width)\n",
    "        Returns:\n",
    "            logits: Tensor of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        patches = self.patchifier(images) # (batch_size, num_patches, num_channels * patch_size * patch_size)\n",
    "        embeddings = self.to_patch_embedding(patches)  # (batch_size, 1 + num_patches, hidden_dim)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        for layer in self.transformer_encoder:\n",
    "            embeddings = layer(embeddings)\n",
    "        embeddings = self.layer_norm(embeddings) # (batch_size, num_patches + 1, embed_dim)\n",
    "\n",
    "        ##### YOUR CODE START #####\n",
    "\n",
    "\n",
    "        ##### YOUR CODE END #####\n",
    "  \n",
    "        return logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "image_size = 224\n",
    "patch_size = 16\n",
    "num_channels = 3\n",
    "embed_dim = 1024\n",
    "num_transformer_layers = 12\n",
    "num_heads = 8\n",
    "feedforward_dim = 3072\n",
    "num_classes = 1000\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "model = ViT(image_size, patch_size, num_channels, embed_dim,\n",
    "            num_transformer_layers, num_heads, feedforward_dim, num_classes)\n",
    "\n",
    "X = torch.randn(batch_size, num_channels, image_size, image_size) # (batch_size, num_channels, height, width)\n",
    "logits = model(X)\n",
    "print(\"ViT output shape:\", logits.shape)\n",
    "\n",
    "assert model(X).shape == (batch_size, num_classes), f\"Expected output shape: ({batch_size}, {num_classes}), but got {logits.shape}\"\n",
    "assert sum(p.numel() for p in model.parameters()) == 127993832, f\"Expected number of model parameter: 86567656, but got {sum(p.numel() for p in model.parameters())}\"\n",
    "\n",
    "print(\"\\033[92m All test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "<mark>실습</mark> 완성한 ViT 모델을 CIFAR-10 데이터셋을 이용하여 학습해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_main():\n",
    "    ## data and preprocessing settings\n",
    "    data_root_dir = '/datasets'\n",
    "    num_workers = 4\n",
    "\n",
    "    ## Training Hyperparameters\n",
    "    num_epochs = 5\n",
    "    batch_size = 64  # Comsumes 1.6GB of GPU memory\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    ## Model hyperparameters\n",
    "    image_size = 32\n",
    "    patch_size = 4\n",
    "    num_channels = 3\n",
    "    embed_dim = 256\n",
    "    num_transformer_layers = 8\n",
    "    num_heads = 4\n",
    "    feedforward_dim = 1024\n",
    "    dropout_prob = 0.1\n",
    "\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(f\"Using {device} device\")\n",
    "\n",
    "    ## Data loaders\n",
    "    train_dataloader, val_dataloader, test_dataloader, num_classes = load_cifar10_dataloaders(\n",
    "        data_root_dir, device, batch_size = batch_size, num_worker = num_workers)\n",
    "    \n",
    "    ## Model, Loss, Optimizer\n",
    "    model = ViT(image_size, patch_size, num_channels, embed_dim,\n",
    "                num_transformer_layers, num_heads, feedforward_dim, \n",
    "                num_classes, dropout_prob = dropout_prob).to(device)\n",
    "    \n",
    "    print(f\"Number of model parameters: {sum(p.numel() for p in model.parameters())}\" +\n",
    "          f\" ({sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable)\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "    ## Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, device, train_dataloader, criterion, optimizer, epoch)\n",
    "        val_loss, val_accuracy = evaluate_one_epoch(model, device, val_dataloader, criterion, epoch)\n",
    "        print(f\"[Epoch {epoch+1:>2}/{num_epochs:<2}] Train Loss: {train_loss:>8.4f}\" +\n",
    "              f\" | Val Loss: {val_loss:>8.4f} | Val Accuracy: {val_accuracy * 100:>4.1f} %\")\n",
    "        \n",
    "    ## Test the model\n",
    "    test_loss, test_accuracy = evaluate_one_epoch(model, device, test_dataloader, criterion, epoch)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:>8.4f} | Test Accuracy: {test_accuracy * 100:>4.1f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "execute_cell"
    ]
   },
   "outputs": [],
   "source": [
    "train_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "코드 구현이 잘 되었다면 별도의 하이퍼파라미터 튜닝(hyperparameter tuning)없이 `Validation Accuracy > 35%`를 달성하실 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
